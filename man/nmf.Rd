% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nmf.R
\name{nmf}
\alias{nmf}
\title{Non-negative matrix factorization}
\usage{
nmf(
  A,
  k,
  tol = 1e-04,
  maxit = 100,
  verbose = TRUE,
  L1 = c(0, 0),
  seed = NULL,
  ...
)
}
\arguments{
\item{A}{matrix of features-by-samples in dense or sparse format (preferred classes are \code{matrix} or \code{Matrix::dgCMatrix}, respectively). Prefer sparse storage when >50\% of values are zero.}

\item{k}{rank}

\item{tol}{stopping criteria for alternating least squares updates, the correlation distance between \eqn{w} across consecutive iterations, \eqn{1 - cor(w_i, w_{i-1})}. Prefer \code{tol < 1e-5} for publication runs, \code{1e-5 < tol < 1e-3} for rapid experimentation.}

\item{maxit}{stopping criteria, maximum number of alternating least squares updates of \eqn{w} and \eqn{h}}

\item{verbose}{print model tolerances between iterations}

\item{L1}{L1/LASSO penalties between 0 and 1, array of length two for \code{c(w, h)}}

\item{seed}{random seed for model initialization. If an array of seeds is provided, multiple factorizations will be run and the model with the lowest Mean Squared Error will be returned.}

\item{...}{see "advanced parameters" section}
}
\value{
A list giving the factorization model:
\itemize{
\item w    : feature factor matrix
\item d    : scaling diagonal vector
\item h    : sample factor matrix
\item tol  : tolerance between models at final update
\item iter : number of alternating updates
}
}
\description{
Sparse matrix factorization of the form \eqn{A = wdh} by alternating least squares with optional non-negativity constraints.
}
\details{
This fast non-negative matrix factorization (NMF) implementation decomposes a matrix \eqn{A} into lower-rank
non-negative matrices \eqn{w} and \eqn{h}, with factors scaled to sum to 1 via multiplication by a diagonal, \eqn{d}: \deqn{A = wdh}

The scaling diagonal ensures convex L1 regularization, consistent factor scalings regardless of random initialization, and model symmetry in factorizations of symmetric matrices.

The factorization model is randomly initialized, and \eqn{w} and \eqn{h} are updated alternately using least squares.

\strong{Parallelization:} Least squares projections in factorizations of rank-3 and greater are parallelized using the number of threads set by \code{\link{setRcppMLthreads}}.
By default, all available threads are used, see \code{\link{getRcppMLthreads}}.
The overhead of parallelization is too great to benefit rank-1 and rank-2 factorization.

\strong{Specializations.} There are specialized backends for dense and sparse matrices (be explicit about what type of matrix you provide), symmetric, rank-1, and rank-2 factorization.

\strong{L1 regularization}. L1 penalization increases the sparsity of factors, but does not change the information content of the model
or the relative contributions of the leading coefficients in each factor to the model. L1 regularization only slightly increases the loss of a model.
L1 penalization should be used to assist interpretability. Penalty values should range from 0 to 1, where 1 gives complete sparsity. In this implementation of NMF,
a scaling diagonal ensures that the L1 penalty is equally applied across all factors regardless of random initialization and the distribution of the model.
Many other implementations of matrix factorization claim to apply L1, but the magnitude of the penalty is at the mercy of the random distribution and
more significantly affects factors with lower overall contribution to the model. L1 regularization of rank-1 and rank-2 factorizations has no effect.

\strong{Publication reference.} Please cite our manuscript: "DeBruine ZJ, Melcher K, Triche TJ (2021)
Fast and robust non-negative matrix factorization for single-cell experiments." on BioRXiv.
}
\section{Advanced Parameters}{

Several parameters may be specified in the \code{...} argument:
\itemize{
\item \code{nonneg = TRUE}: enforce non-negativity
\item \code{diag = TRUE}: scale factors in \eqn{w} and \eqn{h} to sum to 1 by introducing a diagonal, \eqn{d}. This should generally never be set to \code{FALSE}. Diagonalization enables symmetry of models in factorization of symmetric matrices, convex L1 regularization, and consistent factor scalings.
\item \code{mask_zeros = FALSE}: handle zeros as missing values. \code{A} must be a sparse matrix. This algorithm is slower and tolerances will fluctuate more than with the standard algorithm.
\item \code{w_init = NULL}: initial "w" matrix (or list of matrices), supersedes the \code{seed} parameter. If a list of matrices is provided, factorizations will be run for each matrix, and the model with the lowest Mean Squared Error will be returned.
\item \code{update_in_place = FALSE}: avoid transposition of 'A' (and associated memory usage) by using a slower in-place algorithm for alternating least squares updates of 'w'. \code{update_in_place} is always \code{FALSE} when \code{A} is symmetric.
\item \code{samples = 1:ncol(A)}: samples to include in factorization, numbered between 1 and \code{ncol(A)}. Default is all samples. This can make the factorization significantly slower. Generally, prefer subsetting prior to factorization.
\item \code{features = 1:nrow(A)}: features to include in factorization, numbered between 1 and \code{nrow(A)}. Default is all features. This can make the factorization significantly slower. Generally, prefer subsetting prior to factorization.
}
}

\examples{
\dontrun{
library(Matrix)
# basic NMF
model <- nmf(rsparsematrix(1000, 100, 0.1), k = 10)

# compare rank-2 NMF to second left vector in an SVD
data(iris)
A <- as(as.matrix(iris[,1:4]), "dgCMatrix")
nmf_model <- nmf(A, 2, tol = 1e-5)
bipartitioning_vector <- apply(nmf_model$w, 1, diff)
second_left_svd_vector <- base::svd(A, 2)$u[,2]
abs(cor(bipartitioning_vector, second_left_svd_vector))

# compare rank-1 NMF with first singular vector in an SVD
abs(cor(nmf(A, 1)$w[,1], base::svd(A, 2)$u[,1]))

# symmetric NMF
A <- crossprod(rsparsematrix(100, 100, 0.02))
model <- nmf(A, 10, tol = 1e-5, maxit = 1000)
plot(model$w, t(model$h))
# see package vignette for more examples
}
}
\references{
DeBruine, ZJ, Melcher, K, and Triche, TJ. (2021). "High-performance non-negative matrix factorization for large single-cell data." BioRXiv.

Lin, X, and Boutros, PC (2020). "Optimization and expansion of non-negative matrix factorization." BMC Bioinformatics.

Lee, D, and Seung, HS (1999). "Learning the parts of objects by non-negative matrix factorization." Nature.

Franc, VC, Hlavac, VC, Navara, M. (2005). "Sequential Coordinate-Wise Algorithm for the Non-negative Least Squares Problem". Proc. Int'l Conf. Computer Analysis of Images and Patterns. Lecture Notes in Computer Science.
}
\seealso{
\code{\link{nnls}}, \code{\link{project}}, \code{\link{mse}}
}
\author{
Zach DeBruine
}
