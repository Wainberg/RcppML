% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nmf.R
\name{nmf}
\alias{nmf}
\title{Non-negative matrix factorization}
\usage{
nmf(
  A,
  k,
  tol = 0.001,
  maxit = 100,
  verbose = TRUE,
  nonneg = TRUE,
  L1 = c(0, 0),
  seed = NULL,
  threads = 0,
  ...
)
}
\arguments{
\item{A}{dense or sparse matrix of features-by-samples}

\item{k}{rank, or initial matrix for \eqn{w}}

\item{tol}{correlation distance (\eqn{1 - R^2}) between \eqn{w} across consecutive iterations at which to stop factorization}

\item{maxit}{maximum number of alternating updates of \eqn{w} and \eqn{h}}

\item{verbose}{print model tolerances between iterations}

\item{nonneg}{enforce non-negativity}

\item{L1}{L1/LASSO penalties between 0 and 1, array of length two for \code{c(w, h)}}

\item{seed}{random seed for model initialization}

\item{threads}{number of CPU threads, default \code{0} for all available threads}

\item{...}{advanced parameters, see details}
}
\value{
A list giving the factorization model:
\itemize{
\item w    : feature factor matrix
\item d    : scaling diagonal vector
\item h    : sample factor matrix
\item tol  : tolerance between models at final update
\item iter : number of alternating updates run
}
}
\description{
Matrix factorization of the form \eqn{A = wdh} by alternating least squares with optional non-negativity constraints.
}
\details{
This fast non-negative matrix factorization (NMF) implementation decomposes a matrix \eqn{A} into lower-rank
non-negative matrices \eqn{w} and \eqn{h}, with factors scaled to sum to 1 via multiplication by a diagonal, \eqn{d}:
\deqn{A = wdh}.

The scaling diagonal enables symmetric factorization, convex L1 regularization, and consistent factor scalings regardless of random initialization.

The factorization model is randomly initialized, and \eqn{w} and \eqn{h} are updated alternately using least squares.
Given \eqn{A} and \eqn{w}, \eqn{h} is updated according to the equation:

\deqn{w^Twh = wA_j}

This equation is in the form \eqn{ax = b} where \eqn{a = w^Tw} \eqn{x = h} and \eqn{b = wA_j} for all columns \eqn{j} in \eqn{A}.

The corresponding update for \eqn{w} is \deqn{hh^Tw = hA^T_j}.

The alternating least squares projections (see \code{\link{project}} subroutine) are repeated until a stopping criteria is satisfied, which in this implementation is either a maximum number of
iterations or based on the correlation of models between consecutive iterations.

For theoretical and practical considerations, please see our manuscript: "DeBruine ZJ, Melcher K, Triche TJ (2021)
High-performance non-negative matrix factorization for large single cell data." on BioRXiv.

The implementation is carefully optimized for parallelization in high-performance computing environments. There are specializations for dense and sparse, and symmetric
input matrices. Factorizations of rank-1 and -2 are not multithreaded due to excessive overhead.
}
\section{Stopping criteria}{

Use the \code{tol} parameter to control the stopping criteria for alternating updates
\itemize{
\item \code{tol = 1e-2} is appropriate for approximate mean squared error determination and coarse cross-validation, for example in rank determination
\item \code{tol = 1e-3} to \code{1e-4} are suitable for rapid expermentation, cross-validation, and preliminary analysis
\item \code{tol = 1e-5} and smaller for publication-quality runs
}

The \code{maxit} parameter is a secondary stopping criterion that takes effect only if \code{tol} is not satisfied
by the maximum number of specified iterations.
}

\section{Sparse optimization}{

Sparse optimization is automatically applied if \code{A} is a \code{Matrix::dgCMatrix}, or inherits from the \code{Matrix::sparseMatrix} superclass.
Generally, these optimizations are worthwhile if \code{A} is >70\% sparse.

When \code{A} is <70\% sparse, it should be supplied as a dense matrix to avoid the unnecessary overhead of the sparse matrix format.
}

\section{L1 regularization}{

L1 penalization increases the sparsity of factors, but does not change the information content of the model
or the relative contributions of the leading coefficients in each factor. L1 regularization only minorly increases the error of a model.

L1 penalization should be used to aid interpretability as needed. Penalty values should range from 0 to 1, where 1 is
complete sparsity. Note that non-negativity constraints already introduce significant sparsity, so L1 regularization may not always be needed.

In this implementation of NMF, a scaling diagonal ensures that the L1 penalty is equally applied across all factors regardless
of random initialization and the distribution of the model. Many other implementations of matrix factorization claim to apply L1, but
the magnitude of the penalty is at the mercy of the random distribution and more significantly affects factors with lower overall contribution to the model.
}

\section{Rank-2 factorization}{

When \eqn{k = 2}, a very fast optimized algorithm is used.

Two-variable least squares solutions to the problem \eqn{ax = b} are found by direct substitution:
\deqn{x_1 = \frac{a_{22}b_1 - a_{12}b_2}{a_{11}a_{22} - a_{12}^2}}
\deqn{x_2 = \frac{a_{11}b_2 - a_{12}b_1}{a_{11}a_{22} - a_{12}^2}}

In the above equations, the denominator is constant and thus needs to be calculated only once. Additionally, if non-negativity constraints are to be imposed, if \eqn{x_1 < 0} then \eqn{x_1 = 0} and \eqn{x_2 = \frac{b_1}{a_{11}}}.
Similarly, if \eqn{x_2 < 0} then \eqn{x_2 = 0} and \eqn{x_1 = \frac{b_2}{a_{22}}}.

\code{RcppML::nmf} also introduces improved vectorization, compile-time optimization, and performs no transposition of \code{A} for rank-2 factorizations.

L1 regularization of a rank-2 factorization of the form \eqn{A = wdh} has no effect, and thus is not supported.

Results of a rank-2 factorization should be reproducible regardless of random seed.

Rank-2 NMF is useful for bipartitioning, and is a subroutine in \code{\link{bipartition}}, where the sign of the difference between sample loadings
in both factors gives the partitioning.
}

\section{Rank-1 factorization}{

Rank-1 factorization by alternating least squares gives vectors equivalent to the first singular vectors in an SVD. It is a normalization of the data to a middle point,
and may be useful for ordering samples based on the most significant axis of variation (i.e. pseudotime trajectories).

One-variable least squares solutions to the problem \eqn{ax = b} are directly available, thus the implementation can be highly optimized.

Diagonal scaling is applied for consistent linear scaling of both factors across random restarts, and thus always converges to the same solution regardless of random seed.
}

\section{Symmetric factorization}{

Special optimization for symmetric matrices is automatically applied. Specifically, alternating updates of \code{w} and \code{h}
require transposition of \code{A}, but \code{A == t(A)} when \code{A} is symmetric, thus no up-front transposition is performed.
}

\section{Reproducibility}{

Specify a \code{seed} to guarantee absolute reproducibility between restarts. Only random initialization is supported, as other
methods incur unnecessary overhead and sometimes trap updates into local minima.

Because random initializations are used, models may be different across restarts. Factors with lower diagonal weights tend to be most different.
Note that comparable loss values (mean squared error) are achieved even if these models may differ. Finding the exact solution is NP-hard.

In ongoing work, we are exploring robust factorization solution paths using a novel algorithm similar to rank-truncated SVD.
}

\section{Rank determination}{

This function does not attempt to suggest a mechanism for rank determination. Like any clustering algorithm or dimensional reduction,
finding the optimal rank can be subjective. An easy way to
estimate rank uses the "elbow method", where the inflection point on a plot of Mean Squared Error loss (MSE) vs. rank
gives a good idea of the rank at which most of the signal has been captured in the model. Unfortunately, this inflection point
is not often as obvious for NMF as it is for SVD or PCA.

k-fold cross-validation is a better method. Missing value of imputation has previously been proposed, but is arguably no less subjective
than test-training splits and requires computationally
slower factorization updates using missing values, which are not supported here.
}

\section{Advanced parameters}{

The \code{...} argument hides several parameters that may be adjusted, although defaults should entirely satisfy:
\itemize{
\item \code{cd_maxit}, default 1000. See \code{\link{nnls}}.
\item \code{fast_maxit}, default 10. See \code{\link{nnls}}.
\item \code{cd_tol}, default 1e-8. See \code{\link{nnls}}.
\item \code{diag}, set to \code{FALSE} to disable model scaling by the diagonal. When \code{diag = FALSE}, symmetric factorization cannot occur
and L1 regularization is not convex with respect to the factorization model.
}
}

\examples{
library(Matrix)
# basic NMF
model <- nmf(rsparsematrix(1000, 100, 0.1), k = 10)

# compare rank-2 NMF to second left vector in an SVD
A <- iris[,1:4]
nmf_model <- nmf(A, 2, tol = 1e-5)
bipartitioning_vector <- apply(nmf_model$w, 1, diff)
second_left_svd_vector <- base::svd(A, 2)$u[,2]
abs(cor(bipartitioning_vector, second_left_svd_vector))

# compare rank-1 NMF with first singular vector in an SVD
A <- iris[,1:4]
abs(cor(nmf(A, 1)$w[,1], base::svd(A, 2)$u[,1]))

# symmetric NMF
A <- crossprod(rsparsematrix(100, 100, 0.02))
model <- nmf(A, 10, tol = 1e-5, maxit = 1000)
plot(model$w, t(model$h))
# see package vignette for more examples

}
\references{
DeBruine, ZJ, Melcher, K, and Triche, TJ. (2021). "High-performance non-negative matrix factorization for large single-cell data." BioRXiv.

Lin, X, and Boutros, PC (2020). "Optimization and expansion of non-negative matrix factorization." BMC Bioinformatics.

Lee, D, and Seung, HS (1999). "Learning the parts of objects by non-negative matrix factorization." Nature.

Franc, VC, Hlavac, VC, Navara, M. (2005). "Sequential Coordinate-Wise Algorithm for the Non-negative Least Squares Problem". Proc. Int'l Conf. Computer Analysis of Images and Patterns. Lecture Notes in Computer Science.
}
\seealso{
\code{\link{nnls}}, \code{\link{project}}, \code{\link{mse}}, \code{\link{nmf2}}
}
\author{
Zach DeBruine
}
