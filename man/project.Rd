% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/project.R
\name{project}
\alias{project}
\title{Project a linear factor model}
\usage{
project(
  A,
  W = NULL,
  H = NULL,
  nonneg = FALSE,
  L0 = 0,
  L1 = 0,
  L2 = 0,
  RL2 = 0,
  maxit = 100,
  tol = 1e-09,
  L0_method = "eperm",
  precision = "double",
  threads = 0
)
}
\arguments{
\item{A}{square numeric matrix containing the coefficients of the linear system}

\item{W}{numeric vector or matrix giving the right-hand side(s) of the linear system}

\item{H}{optional initial numeric vector for x (if using exclusively coordinate descent)}

\item{nonneg}{apply non-negativity constraints}

\item{L0}{L0 truncation to be incrementally enforced for each column in b. Use 0 or -1 to apply no penalty.}

\item{L1}{L1/LASSO regularization to be subtracted from \eqn{b}. Generally this should be scaled to \code{max(b)}}

\item{L2}{L2/Ridge regression to be added to the diagonal of \eqn{a}. Generally this should be scaled to \code{mean(diag(a))} or similar.}

\item{RL2}{Reciprocal L2/angular/Pattern Extraction penalty to be added to off-diagonal of \eqn{a}. Generally this should be scaled as in \eqn{L2}.}

\item{maxit}{maximum number of iterations for least squares sequential coordinate descent}

\item{tol}{tolerance for convergence of the least squares sequential coordinate descent solver}

\item{L0_method}{algorithm for solving L0-regularized solution, one of "convex", "ggperm", "eperm", or "exact". See details.}

\item{precision}{"float" or "double"}

\item{threads}{number of cPU threads to use for parallelization when \eqn{b} is a matrix. Default is "0" to let OpenMP decide and use all available threads.}
}
\value{
matrix giving either \eqn{W} or \eqn{H}
}
\description{
Solves the equation \eqn{A = WH} for either \eqn{W} or \eqn{H}
}
\details{
For the classical alternating matrix factorization update problem \eqn{A = WH}, the updates
(or projection) of \eqn{H} is given by the systems of equations: \deqn{W^TWH = WA_j}
\deqn{a = W^TW} \deqn{x = H} \deqn{b = WA_j} for all columns \eqn{j} in \eqn{A}.

Given \eqn{A} and \eqn{w}, \code{RcppML::project} solves for \eqn{H} using the above formulas and
\code{RcppML::solve}, applying regularizations up-front and parallelizing solutions across columns in
\eqn{A}.

The corresponding equation for updating \eqn{W} in block-pivoting is: \deqn{HH^TW^T = HA^T_j}

See \code{\link{solve}} for details on NNLS and L0 regularization.
}
