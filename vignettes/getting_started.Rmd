---
title: "Getting Started with NMF"
author: "Zach DeBruine"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with NMF}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette demonstrates basic usage of the `RcppML::nmf` function and visualization of the results.

## Install RcppML

Install the RcppML R package from CRAN or the development version from GitHub. 

Also install the accompanying Machine Learning datasets (MLdata) package:

```{R, eval = FALSE}
install.packages('RcppML')                     # install CRAN version
# devtools::install_github("zdebruine/RcppML") # compile dev version
devtools::install_github("zdebruine/MLdata")
```

## What is NMF?

Non-negative Matrix Factorization (NMF) finds additive signals in non-negative data in terms of the features and samples associated with those signals.

NMF gives an approximation of an input matrix as the cross-product of two low-rank submatrices: 

$$A = wdh$$

Here, $A$ is the input matrix, $w$ is a tall matrix of features in rows and factors in columns, and $h$ is a wide matrix of factors in rows and samples in columns. 

`RcppML::nmf` introduces one more important component into this system, a scaling diagonal, $d$. This scaling diagonal provides:

* consistent factor scalings throughout model fitting
* robustness across random restarts
* symmetry in factorization of symmetric matrices
* a means for convex L1 regularization

## Running NMF

Run NMF on the `iris` dataset. We need to specify a rank (`k`) for the factorization, and will also specify the `seed` for random initialization for reproducibility:

```{R, message = FALSE, warning = FALSE, results = "hide"}
library(RcppML)
library(Matrix)
library(MLdata)
library(ggplot2)
library(cowplot)

data(iris)
model <- nmf(iris[,1:4], k = 3, seed = 1)
```

```{R}
model
```

## Visualizing NMF Models

The result of `RcppML::nmf` is an S3 object of class `nmf`. The `nmf` class has many useful methods:

```{R, warning = FALSE}
methods(class = "nmf")
```

One of these useful methods is `summary` (which in turn has a `plot` method):

```{R}
species_stats <- summary(model, group_by = iris$Species)
species_stats
```
```{R, fig.height = 2.5, fig.width = 3}
plot(species_stats, stat = "sum")
```

Notice how NMF factors capture variable information among iris species.

The `biplot` method for NMF (see `?biplot.nmf` for details) can compare the weights of different features or samples in two factors:

```{R, fig.height = 3, fig.width = 4}
biplot(model, factors = c(1, 2), group_by = iris$Species)
```

## Random Restarts

NMF is randomly initialized, thus results may be slightly different every time. To run NMF many times, set multiple seeds, and the best model will be returned. 

Here we run 10 factorizations at a higher tolerance, and the best model is returned:

```{R, results = "hide"}
model2 <- nmf(iris[,1:4], k = 3, seed = 1:10, tol = 1e-5)
```

```{R}
# MSE of model from single random initialization
evaluate(model, iris[,1:4])

# MSE of best model among 10 random restarts
evaluate(model2, iris[,1:4])
```

The second model is slightly better.

## L1 Regularization

Sparse factors contain only a few non-zero values and make it easy to identify features or samples that are important. 

L1/LASSO regularization is the best method for introducing sparsity into a linear model.

```{R, results = "hide"}
data(movielens)
ratings <- movielens$ratings
model_L1 <- nmf(ratings, k = 7, L1 = 0.1, seed = 123, mask_zeros = TRUE)
```

```{R}
sparsity(model_L1)
```

The `sparsity` S3 method for class `nmf` makes it easy to compute the sparsity of factors, as done above.

Note that `mask_zeros = TRUE` in the example above. This is because zero-valued ratings are missing, and thus should not be considered during factorization.

In the above example, we regularized both $w$ and $h$, however each model can also be regularized separately:

```{R, results = "hide"}
model_no_L1 <- nmf(ratings, k = 7, L1 = 0, seed = 123, mask_zeros = TRUE)
model_L1_h <-  nmf(ratings, k = 7, L1 = c(0, 0.1), seed = 123, mask_zeros = TRUE)
model_L1_w <-  nmf(ratings, k = 7, L1 = c(0.1, 0), seed = 123, mask_zeros = TRUE)

# summarize sparsity of all models in a data.frame
df <- rbind(sparsity(model_no_L1), sparsity(model_L1_h), sparsity(model_L1_w), sparsity(model_L1))
df$side <- c(rep("none", 14), rep("h only", 14), rep("w only", 14), rep("both", 14))
df$side <- factor(df$side, levels = unique(df$side))
```

```{R, fig.height = 3, fig.width = 4}
ggplot(df, aes(x = side, y = sparsity, color = model)) + 
  geom_boxplot(outlier.shape = NA, width = 0.6) + 
  geom_point(position = position_jitterdodge()) + theme_classic() + 
  labs(x = "Regularized side of model", y = "sparsity of model factors")
```

Note how each side of the model is regularized independently.

L1 regularization does not significantly affect model loss:

```{R}
# L1 = 0
evaluate(model_no_L1, movielens$ratings, mask = "zeros")

# L1 = 0.1
evaluate(model_L1, movielens$ratings, mask = "zeros")
```

L1 regularization also does not significantly affect model information at low penalties. Here we measure the cost of bipartite matching between two models on a cosine distance matrix for `L1 = 0`, `L1 = 0.01`, and `L1 = 0.1`:

```{R, results = "hide"}
model_low_L1 <- nmf(movielens$ratings, k = 5, L1 = 0.01, seed = 123)
```

```{R}
# cost of bipartite matching: L1 = 0 vs. L1 = 0.01
bipartiteMatch(1 - cosine(model_no_L1$w, model_low_L1$w))$cost / 10

# cost of bipartite matching: L1 = 0 vs. L1 = 0.1
bipartiteMatch(1 - cosine(model_no_L1$w, model_L1$w))$cost / 10
```

See `?RcppML::cosine` for details on very fast computation of cosine similarity.

In the above code, we computed cosine distance by subtracting cosine similarity from 1, matched on this cost matrix, and divided by 10 to find the mean cosine distance between matched factors. In both cases, factors correspond well.

Thus, regularized `RcppML::nmf` increases factor sparsity without significantly affecting the loss or information content of the model.

## Prediction/Recommendation with NMF

NMF models learned on some samples can be projected to other samples, a common routine in recommender systems or transfer learning. 

For instance, we may train a model on movie ratings from many users in the `movielens` dataset (training users) and predict ratings for the remaining users (test users).

```{R, results = "hide"}
train_users <- sample(1:ncol(ratings), 500)

# remove movies with fewer than 5 ratings in the training set
movies <- which(rowSums(ratings[, train_users]) > 5)
model <- nmf(ratings[movies, train_users], k = 10, mask_zeros = TRUE)
predictions <- predict(model, ratings[movies, -train_users])
```

Now we can assess the ability of our model to predict movie ratings by users in the test set. Because we trained the model with `mask_zeros = TRUE`, we also need to do the same when calculating mean squared error:

```{R}
evaluate(new("nmf", w = model@w, d = rep(1, 10), h = predictions), ratings[movies, -train_users], mask = "zeros")
```

## Cross-validation for rank determination

Cross-validation can assist in finding a reasonable factorization rank. However, like many dimensional reductions, a single "best" rank rarely exists.

We will demonstrate cross-validation using two simulated datasets generated with `simulateNMF`:

1. `data_clean` will have no noise or signal dropout
2. `data_dirty` contains the same signal as `data_clean`, but with a bit of noise and a lot of dropout

```{R}
data_clean <- simulateNMF(nrow = 100, ncol = 100, k = 5, noise = 0, dropout = 0, seed = 123)
data_dirty <- simulateNMF(nrow = 100, ncol = 100, k = 5, noise = 0.25, dropout = 0.4, seed = 123)
```

Notice how `data_clean` contains only 5 non-zero singular values, while `data_dirty` does not:

```{R, fig.width = 3, fig.height = 3, echo = FALSE}
df <- data.frame("singular_value" = svd(data_clean)$d[1:10], "k" = 1:10, "dataset" = rep("clean", 10))
df2 <- data.frame("singular_value" = svd(data_dirty)$d[1:10], "k" = 1:10, "dataset" = rep("dirty", 10))

ggplot(rbind(df, df2), aes(x = k, y = singular_value, color = dataset)) + 
  geom_point() + 
  geom_line() + 
  theme_classic() + 
  scale_x_continuous(breaks = c(2, 4, 6, 8, 10)) + 
  labs(x = "singular value", y = "standard deviation") +
  theme(aspect.ratio = 1)
```

We can use `RcppML::crossValidate` to determine the rank of each dataset. The default method uses "bi-cross-validation". See `?crossValidate` for details.

```{R, fig.width = 6, fig.height = 3}
cv_clean <- crossValidate(data_clean, k = 1:10, seed = 123)
cv_dirty <- crossValidate(data_dirty, k = 1:10, seed = 123)
plot_grid(
  plot(cv_clean) + ggtitle("cross-validation on\nclean dataset") + scale_y_continuous(limits = c(0, 0.5)),
  plot(cv_dirty) + ggtitle("cross-validation on\ndirty dataset"), nrow = 1)
```

`crossValidate` also supports another method which compares robustness of two factorizations on independent sample subsets.

```{R, fig.width = 6, fig.height = 3, warning = FALSE}
cv_clean <- crossValidate(data_clean, k = 2:10, method = "robust", seed = 123)
cv_dirty <- crossValidate(data_dirty, k = 2:10, method = "robust", seed = 123)
plot_grid(
  plot(cv_clean) + ggtitle("cross-validation on\nclean dataset"),
  plot(cv_dirty) + ggtitle("cross-validation on\ndirty dataset"), nrow = 1)
```

For real datasets, it is important to experiment with all cross-validation methods and to explore multi-resolution analysis or other objectives where appropriate.

`crossValidate` does not support an objective against the mean squared error of imputed missing values because this method is slow, tends to under-estimate the true rank, and does not handle noisy data well.