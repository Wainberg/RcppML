---
title: "Getting Started with NMF"
author: "Zach DeBruine"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to the RcppML package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette introduces the theory behind NMF, data preparation, and provides several examples of how to run NMF using the RcppML library.

## What is NMF?

Non-negative Matrix Factorization (NMF) gives an approximation of the input matrix as the cross-product of two low-rank submatrices:

\deqn{A = wh}

Here, \eqn{A} is the input matrix, \eqn{w} is a tall matrix of features in rows and factors in columns, and \eqn{h} is a wide matrix of factors in rows and samples in columns.

RcppML NMF introduces one more important component into this system, a scaling diagonal, \eqn{d}:

\deqn{A = wdh}

Much like in truncated SVD, this scaling diagonal ensures that factors are always scaled consistently, thus ensuring robustness across random restarts, symmetry in factorization of symmetric matrices, and enabling convex L1 regularization.

## Prepping data for factorization

NMF requires a matrix of features (rows) by samples (columns), with input values either observations or some normalized or transformed representation of these observations.

In this vignette we will use the \code{\link{HawaiiBirds}} dataset:

```{R}
data(HawaiiBirds)
str(HawaiiBirds)
```

This data describes mean bird counts (`$counts` sparse matrix) observed in square kilometer grids throughout Hawaii, with information about the location of each of these grids (`$metadata` data frame). Sampling is incomplete, because birds in all grids have been exhaustively surveyed, thus zeros may indicate either missing or true information. One of the strengths of NMF is the ability to deal with this type of information by imputing data that is likely missing.

We will not normalize or transform this count data because NMF handles unevenly distributed information quite well.

Like many applications for NMF, this bird count data is naturally non-negative. If your data can contain negative values, you can still run \code{RcppML::nmf}, but without non-negativity constraints (see "Advanced Parameters", set \code{nonneg = FALSE}).

## Running NMF

We will use the \code{RcppML::nmf} function. For starters, we will use a rank of 10:

```{R, message = FALSE}
library(RcppML)
nmf_model <- RcppML::nmf(HawaiiBirds$counts, k = 10)
```

Let's take a look at the result:

```{R}
str(nmf_model)
```

Here we have a \eqn{w} and \eqn{h} matrix, along with a scaling diagonal \eqn{d}. In the case of our bird data, \eqn{w} gives the composition of bird communities, \eqn{h} gives the representation of each of these "communities" in each geographical grid, and \eqn{d} gives the relative importance of each factor to the overall model.

## Interpreting NMF

Let's take a look at factor 1, the most important factor in the overall model. Where is factor 1 geographically represented?

```{R}
library(ggplot2)
ggplot2()
```

## Different Flavors of NMF

## Finding the best rank


#' Given \eqn{A} and \eqn{w}, \eqn{h} is updated according to the equation: \deqn{w^Twh = wA_j}
#'
#' This equation is in the form \eqn{ax = b} where \eqn{a = w^Tw}, \eqn{x = h}, and \eqn{b = wA_j} for all columns \eqn{j} in \eqn{A}.
#'
#' The corresponding update for \eqn{w} is \deqn{hh^Tw = hA^T_j}


#' **Rank-2 factorization.** When \eqn{k = 2}, a very fast optimized algorithm is used. Two-variable least squares solutions to the problem \eqn{ax = b} are found by direct substitution:
#' \deqn{x_1 = \frac{a_{22}b_1 - a_{12}b_2}{a_{11}a_{22} - a_{12}^2}}
#' \deqn{x_2 = \frac{a_{11}b_2 - a_{12}b_1}{a_{11}a_{22} - a_{12}^2}}
#'
#' In the above equations, the denominator is constant and thus needs to be calculated only once. Additionally, if non-negativity constraints are to be imposed, 
#' if \eqn{x_1 < 0} then \eqn{x_1 = 0} and \eqn{x_2 = \frac{b_1}{a_{11}}}. 
#' Similarly, if \eqn{x_2 < 0} then \eqn{x_2 = 0} and \eqn{x_1 = \frac{b_2}{a_{22}}}.
#'
#' Rank-2 NMF is useful for bipartitioning, and is a subroutine in \code{\link{bipartition}}, where the sign of the difference between sample loadings
#' in both factors gives the partitioning.


#'
#' **Rank-1 factorization.** Rank-1 factorization by alternating least squares gives vectors equivalent to the first singular vectors in an SVD. It is a normalization of the data to a middle point, 
#' and may be useful for ordering samples based on the most significant axis of variation (i.e. pseudotime trajectories). Diagonal scaling guarantees consistent linear 
#' scaling of the factor across random restarts.
#'


#' **Rank determination.** This function does not attempt to provide a method for rank determination. Like any clustering algorithm or dimensional reduction,
#' finding the optimal rank can be subjective. An easy way to 
#' estimate rank uses the "elbow method", where the inflection point on a plot of Mean Squared Error loss (MSE) vs. rank 
#' gives a good idea of the rank at which most of the signal has been captured in the model. Unfortunately, this inflection point
#' is not often as obvious for NMF as it is for SVD or PCA.
#' 
#' k-fold cross-validation is a better method. Missing value of imputation has previously been proposed, but is arguably no less subjective 
#' than test-training splits and requires computationally slower factorization updates using missing values, which are not supported here.
#'



## Running NMF

## Interpreting the Results

## 

The 'RcppML' package provides high-performance machine learning algorithms using Rcpp with a focus on matrix factorization.

## Installation

Install the latest development version of RcppML from github:

```{R, eval = FALSE}
library(devtools)
install_github("zdebruine/RcppML")
```

```{R}
library(RcppML)
library(Matrix)
```

## Non-Negative Least Squares

RcppML contains extremely fast NNLS solvers. Use the `nnls` function to solve systems of equations subject to non-negativity constraints.

The `RcppML::solve` function solves the equation \eqn{ax = b} for \eqn{x} where \eqn{a} is symmetric positive definite matrix of dimensions \eqn{m x m} and \eqn{b} is a vector of length \eqn{m} or a matrix of dimensions \eqn{m x n}.

```{R}
# construct a system of equations
X <- matrix(rnorm(2000),100,20)
btrue <- runif(20)
y <- X %*% btrue + rnorm(100)
a <- crossprod(X)
b <- crossprod(X, y)

# solve the system of equations
x <- RcppML::nnls(a, b)

# use only coordinate descent
x <- RcppML::nnls(a, b, fast_nnls = FALSE, cd_maxit = 1000, cd_tol = 1e-8)
```

`RcppML::solve` implements a new and fastest-in-class algorithm for non-negative least squares:

1. *initialization* is done by solving for the unconstrained least squares solution.
2. *forward active set tuning* (FAST) provides a near-exact solution (often exact for well-conditioned systems) by setting all negative values in the unconstrained solution to zero, re-solving the system for only positive values, and repeating the process until the solution for values not constrained to zero is strictly positive. Set `cd_maxit = 0` to use only the FAST solver.
3. *Coordinate descent* refines the FAST solution and finds the best solution discoverable by gradient descent. The coordinate descent solution is only used if it gives a better error than the FAST solution. Generally, coordinate descent re-introduces variables constrained to zero by FAST back into the feasible set, but does not dramatically change the solution.

## Projecting Linear Models

Project dense linear factor models onto real-valued sparse matrices (or any matrix coercible to `Matrix::dgCMatrix`) using `RcppML::project`. 

`RcppML::project` solves the equation \eqn{A = WH} for \eqn{H}.

```{R}
# simulate a sparse matrix
A <- rsparsematrix(1000, 100, 0.1)

# simulate a linear factor model
w <- matrix(runif(1000 * 10), 1000, 10)

# project the model
h <- RcppML::project(A, w)
```

## Non-negative Matrix Factorization

`RcppML::nmf` finds a non-negative matrix factorization by alternating least squares (alternating projections of linear models \eqn{w} and \eqn{h}).

There are several ways in which the NMF algorithm differs from other currently available methods:

* Diagonalized scaling of factors to sum to 1, permitting convex L1 regularization along the entire solution path
* Fast stopping criteria, based on correlation between models across consecutive iterations
* Extremely fast algorithms using the Eigen C++ library, optimized for matrices that are >90% sparse
* Support for NMF or unconstrained matrix factorization
* Parallelized using OpenMP multithreading

The following example runs rank-10 NMF on a random 1000 x 1000 matrix that is 90% sparse:

```{R}
A <- rsparsematrix(100, 100, 0.1)
model <- RcppML::nmf(A, 10, verbose = F)

w <- model$w
d <- model$d
h <- model$h
model_tolerance <- tail(model$tol, 1)
```

Tolerance is simply a measure of the average correlation between \eqn{w_{i-1} and \eqn{w_i} and \eqn{h_{i-1}} and \eqn{h_i} for a given iteration \eqn{i}.

For symmetric factorizations (when \code{symmetric = TRUE}), tolerance becomes a measure of the correlation between \eqn{w_i} and \eqn{h_i}, and diagonalization is automatically performed to enforce symmetry:

```{R}
A_sym <- as(crossprod(A), "dgCMatrix")

model <- RcppML::nmf(A_sym, 10, verbose = F)
```

Mean squared error of a factorization can be calculated for a given model using the `RcppML::mse` function:

```{R}
RcppML::mse(A_sym, model$w, model$d, model$h)
```
