---
title: "Figure 1"
author: "Zach DeBruine"
date: "7/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Towards High-Performance NMF

Our objective is to develop a high-performance implementation of non-negative matrix factorization capable of handling an extremely large sparse matrix (>90% sparse). Current implementations of NMF are very slow, making it difficult to process large datasets and challenging widespread adoption of the method. For example, there are SVD algorithms which are an order of magnitude faster than NMF (i.e. implicitly-restarted lanczos bidiagonalization algorithm, "irlba") even though the computational complexity of matrix factorization by alternating least squares is roughly equivalent.

Alternating least squares is the fastest method for orthogonal factorization. Code profiling reveals several bottlenecks in current methods: calculation of the right hand side of linear systems for updates of "w" and "h", solving the linear systems using non-negative least squares, and calculating the measure to be used as stopping criteria. The methods used to address the first bottleneck are discussed in the "NMF implementation" in the methods section, and deals with sparse matrix support and proper vectorization. We will address the other two bottlenecks here.

## Methods for solving non-negative least squares

Coordinate descent least squares are attractive because \eqn{a} need not be positive definite. Different initializations for the coordinate descent solver might affect the number of iterations required for convergence. We examined whether initialization with a zero-filled vector, an appropriately-scaled uniform random distribution, or an unconstrained solution affected runtime.

For random systems of equations between 10 and 100 variables in size, there were no differences in runtime between zero or random initializations. For initialization with unconstrained solutions, smaller systems (<50 variables) had a small advantage while larger systems (>50 variables) surprisingly had a slight disadvantage. Note that the time required to calculate the unconstrained solution was not included in the benchmark. Thus, zero-initialization is a satisfactory method for coordinate descent least squares.

In the ultimate test of NNLS, we simulated random systems of equations in which there are no intentional relationships between variables. Simply, a random matrix "X" and vector "y" is constructed, and "a" is the crossproduct of "X" and "b" is the crossproduct of "X" and "y".


## Coordinate Descent Least Squares

Coordinate descent algorithms require a starting point, or an initial solution "suggestion", from which to descend down the gradient of residuals towards the true solution. Some obvious initializations might include a random or zero-filled vector, or an unconstrained least squares solution.

Coordinate descent might also be primed with a sparse solution . This methodology is similar to the concept presented by Myre et al. in the "TNT-NN" least squares algorithm, but uses only the first step of their proposed method, avoiding the hyperparameters and heuristics in the second step.

Finally, and quite simply, we might initialize coordinate descent with a vector filled with a random uniform distribution or zeros.

* Coordinate descent is the best-performing algorithm for solving non-negative least squares problems
* Choice of initialization does not significantly affect coordinate descent convergence speed
* Fast active set tuning is an attractive alternative to coordinate descent least squares, but does not outperform it and requires a positive definite \eqn{a}.
* RcppML \code{nnls} implementation is faster than the Lawson-Hanson fortran77 implementation, and competes well with unconstrained least squares solutions (compared to Eigen Cholesky solver).



## Methods 

### NMF Implementation

Our work was inspired by a previous publication which demonstrated the utility of sequential coordinate descent least squares in matrix factorization. We started by building a similar algorithm from scratch, with several major changes. First, we used the Eigen C++ library for efficient vectorization and industry-leading crossproduct performance. Second, we added sparse matrix support with Eigen sparse matrix iterators used to calculate the right-hand side of systems of equations. 

## Towards fast NNLS for matrix factorization

Systems of equations that appear in NMF differ from those that might appear as one-off problems (i.e. model projection) in that they are well-conditioned and unconstrained solutions tend to give non-negative results. Furthermore, solutions between consecutive iterations are very similar, suggesting that an algorithm which can make use of the previous solution as a starting point will have a benefit.

In the case of matrix factorization where solutions tend to be similar to one another across consecutive iterations, an obvious initialization for coordinate descent is the previous solution.

```{R}
library(RcppML)
library(reshape2)
library(ggplot2)
library(microbenchmark)
library(NNLM)
library(irlba)
```


## Figure 1: A high-performance implementation of Non-Negative Matrix Factorization for large sparse matrices

A) Runtime of a rank-10 factorization of a 1000 x 10000 random 90% sparse matrix using RcppML::NMF, NNLM::nnmf, and irlba. Runtime was measured for factorizations with 100 different random initializations. Convergence for NMF was called at two tolerances (1e-3, 1e-4).

```{R}
A <- abs(rsparsematrix(1000, 10000, 0.1))
A_dense <- as.matrix(A)
mb <- microbenchmark(
  "irlba" = irlba::irlba(A, 10),
  "RcppML (tol = 1e-3)" = RcppML::nmf(A, 10, tol = 1e-3, verbose = FALSE, diag = FALSE),
  "RcppML (tol = 1e-4)" = RcppML::nmf(A, 10, tol = 1e-4, verbose = FALSE, diag = FALSE),
  "NNLM (tol = 1e-3)" = NNLM::nnmf(A_dense, 10, n.threads = 0, rel.tol = 1e-3, verbose = FALSE),
  "NNLM (tol = 1e-4)" = NNLM::nnmf(A_dense, 10, n.threads = 0, rel.tol = 1e-4, verbose = FALSE),
  times = 3, unit = 's'
)

autoplot(mb) + theme_classic() + theme(aspect.ratio = 1)
```

B) Same as in (A) but for factorizations with three different random initializations for a single random sparse matrix at ranks between 2 and 30.

```{R}
results <- data.frame()
for(rank in seq(from = 2, to = 30, by = 1)){
  cat("Calculating runtime for rank: ", rank, "\n")
  df <- summary(microbenchmark(
      RcppML::nmf(A, rank, verbose = FALSE, diag = FALSE, tol = 1e-3),
      RcppML::nmf(A, rank, verbose = FALSE, diag = FALSE, tol = 1e-4),
      NNLM::nnmf(A_dense, rank, n.threads = 0, rel.tol = 1e-3, verbose = FALSE),
      irlba(A, rank),
      times = 1, unit = 's'
    ))$mean
  names(df) <- c("RcppML (tol = 1e-3)", "RcppML (tol = 1e-4)", "NNLM (tol = 1e-3)", "irlba")
  df < data.frame(df)
  ifelse(length(results) == 0, results <- df, results <- rbind(results, df))
}
rownames(results) <- seq(from = 2, to = 30, by = 1)
results <- as.matrix(results)
results <- reshape2::melt(results)
colnames(results) <- c("rank", "method", "time")

ggplot(results, aes(x = rank, y = time, color = method, group = method)) +
    geom_point() +
    geom_line(size = 1) +
    theme_classic() +
    scale_y_continuous(expand = c(0.01, 0)) +
    theme(aspect.ratio = 1) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "factorization rank", y = "runtime (s)")
```

C) RcppML NMF scales well to datasets containing large numbers of samples, as shown by runtime for a rank-10 factorization of a 99% sparse  random matrix containing 1000 features and between 100 to 1 million samples for a rank-10 factorization.

```{R}
results <- data.frame()
sizes <- c(100, 250, 500, 750, 1000, 2500, 5000, 7500, 10000, 25000, 50000, 75000, 100000, 500000, 1000000)
for(num_samples in sizes){
  cat("Calculating runtime for num_samples: ", num_samples, "\n")
  A <- rsparsematrix(1000, num_samples, 0.01)
  df <- summary(microbenchmark(
      RcppML::nmf(A, 10, verbose = FALSE, diag = FALSE, tol = 1e-4),
      irlba(A, 10),
      times = 1, unit = 's'
    ))$mean
  names(df) <- c("RcppML (tol = 1e-4)", "irlba")
  df < data.frame(df)
  ifelse(length(results) == 0, results <- df, results <- rbind(results, df))
}
rownames(results) <- sizes
results <- as.matrix(results)
results <- reshape2::melt(results)
colnames(results) <- c("num_samples", "method", "time")

ggplot(results, aes(x = num_samples, y = time, color = method, group = method)) +
    geom_point() +
    geom_line(size = 1) +
    theme_classic() +
    scale_y_continuous(expand = c(0.01, 0), trans = 'log10') + scale_x_continuous(trans = 'log10') +
    theme(aspect.ratio = 1) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "number of samples", y = "runtime (s)")
```

D) Two-layer factorization of 1 million single-cell transcriptomes (16,543 genes x 1,254,342 cells) to learn patterns of coordinated gene activity.

E) GO term enrichment in patterns

## Figure 2: A novel algorithm for fast non-negative least Squares

A) solution path for zero-initialized coordinate descent NNLS
B) solution path for FAST-initialized coordinate descent NNLS
C) runtime of NNLS with various initializations
D) runtime of NNLS compared to gold-standard implementations
E) runtime of NMF with various NNLS algorithms
F) NMF does not require coordinate descent for convergence (tolerances over iterations for CD, FAST-CD, and FAST)

```{R}
autoplot(microbenchmark(
  "FAST" = RcppML::nmf(A, rank, verbose = FALSE, diag = FALSE, tol = 1e-4, cd_maxit = 0),
  "CD-FAST" = RcppML::nmf(A, rank, verbose = FALSE, diag = FALSE, tol = 1e-4),
  "CD" = RcppML::nmf(A, rank, verbose = FALSE, diag = FALSE, tol = 1e-4, fast_maxit = 0),
  times = 5
))
```

## Figure 3: Model diagonalization is required for symmetric factorization and L1 regularization

A) Factorization without diagonalization results in W and H with different distributions and factor scalings (plot of W vs. H, histograms of W and H distributions). Distribution depends on random start.

B) Factorization with diagonalization results in W and H with same factor scalings (plot of W vs. H) and diagonal scalings

C) sparsity of factors in L1 regularized model without diagonalization (NNLM nnmf function)

D) sparsity of factors in L1 regularized model with diagonalization (RcppML NMF function)

## Figure 4: Fast stopping criteria for matrix factorization

A) Pearson correlation between c(w_i-1 and w_i) and mean squared error of model

B) Pearson correlation between c(w_i-1 and w_i vs. w_i and h_i in a symmetric factorization)

```{R}
A <- as(crossprod(rsparsematrix(1000, 1000, 0.01)), "dgCMatrix")
mod <- RcppML::nmf(A, 10, tol = 1e-7, verbose = FALSE, maxit = 1000, calc_mse = TRUE)
mse_tol <- (2 * (mod$mse[1:length(mod$mse)-1] - mod$mse[2:(length(mod$mse))])) / (mod$mse[2:length(mod$mse)] + mod$mse[1:(length(mod$mse) - 1)])
mod_tol <- mod$tol[2:length(mod$tol)]
df <- data.frame("cor" = mod_tol, "mse" = mse_tol)

ggplot(df, aes(x = cor, y = mse)) +
    geom_point() +
    theme_classic() +
    scale_y_continuous(expand = c(0.01, 0), trans = 'log10') + scale_x_continuous(trans = 'log10') +
    theme(aspect.ratio = 1) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "tolerance (correlation)", y = "tolerance (mse)")

results <- data.frame()
sizes <- c(100, 250, 500, 750, 1000, 2500, 5000, 7500, 10000)
for(num_samples in sizes){
  cat("Calculating runtime for num_samples: ", num_samples, "\n")
  A <- rsparsematrix(1000, num_samples, 0.01)
  df <- summary(microbenchmark(
      RcppML::nmf(A, 10, verbose = FALSE, diag = FALSE, tol = 1e-4, calc_mse = FALSE),
      RcppML::nmf(A, 10, verbose = FALSE, diag = FALSE, tol = 1e-4, calc_mse = TRUE),
      times = 5, unit = 's'
    ))$mean
  names(df) <- c("without MSE calculation", "with MSE calculation")
  df < data.frame(df)
  ifelse(length(results) == 0, results <- df, results <- rbind(results, df))
}
rownames(results) <- sizes
results <- as.matrix(results)
results <- reshape2::melt(results)
colnames(results) <- c("num_samples", "method", "time")

ggplot(results, aes(x = num_samples, y = time, color = method, group = method)) +
    geom_point() +
    geom_line(size = 1) +
    theme_classic() +
    scale_y_continuous(expand = c(0.01, 0), trans = 'log10') + scale_x_continuous(trans = 'log10') +
    theme(aspect.ratio = 1) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "number of samples", y = "runtime (s)")

```

Correlation between wh vs w_i-1 and w_i:

```{R}
set.seed(1)
A <- as(crossprod(rsparsematrix(1000, 1000, 0.01)), "dgCMatrix")
wh_cor <- RcppML::nmf(A, 10, tol = 1e-10, verbose = FALSE, maxit = 1000)$tol
ww_cor <- RcppML::nmf(A, 10, tol = 1e-10, verbose = FALSE, maxit = 1000, symmetric = TRUE)$tol

df <- data.frame("ww_cor" = ww_cor[1:100], "wh_cor" = wh_cor[1:100])

ggplot(df, aes(x = ww_cor, y = wh_cor)) +
    geom_point() +
    theme_classic() +
    scale_y_continuous(expand = c(0.01, 0), trans = 'log10') + scale_x_continuous(trans = 'log10') +
    theme(aspect.ratio = 1) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "cor(w, h)", y = "cor(w, w)")

```

## Figure 5: Matrix factorization of 1 million single-cell transcriptomes on a desktop computer.

```{R}
random_system <- function(m, seed){
  set.seed(seed)
  X <- abs(matrix(rnorm(100*m),100, m))
  btrue <- runif(m)
  y <- X %*% btrue + rnorm(100)
  return(list("a" = crossprod(X), "b" = crossprod(X, y), "X" = X, "y" = y))
}
```




```{R}
results <- data.frame()
for(size in seq(from = 10, to = 100, by = 10)){
  cat("Calculating runtime for size: ", size, "\n")
  df <- list()
  for(replicate in 1:3){
    ab <- random_system(size, replicate)
    solution <- RcppML::nnls(ab$a, ab$b)
    x_zero <- as.matrix(rep(0, size))
    x_rand <- as.matrix(runif(length(ab$b))) * mean(solution)
    x_solve <- base::solve(ab$a, ab$b)
    df[[replicate]] <- summary(microbenchmark(
      RcppML::nnls(ab$a, ab$b, x = x_zero, cd_maxit = 100000),
      RcppML::nnls(ab$a, ab$b, x = x_rand, cd_maxit = 100000),
      RcppML::nnls(ab$a, ab$b, fast_maxit = 0, cd_maxit = 100000),
      RcppML::nnls(ab$a, ab$b, fast_maxit = 100, cd_maxit = 100000),
      times = 3, unit = 's'
    ))$mean
  }
  df <- do.call(rbind, df)
  time <- colMeans(df)
  err <- apply(df, 2, sd) / time[1]
  time <- time / time[1]
  df <- data.frame(cbind(time, err))
  df$size <- size
  df$method <- c("zero", "random", "unconstrained", "FAST")
  ifelse(nrow(results) == 0, results <- df, results <- rbind(results, df))
}

ggplot(results, aes(x = size, y = time, color = method, group = method)) +
    geom_point() +
    geom_line(size = 1) +
    geom_errorbar(aes(ymin = time - err/2, ymax = time + err/2), width = 0.1) +
    theme_classic() +
    scale_y_continuous(expand = c(0.01, 0)) +
    theme(aspect.ratio = 1) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "number of variables in system", y = "runtime relative to zero-initialized")
```

For well-conditioned systems, however, we might expect different behavior. Here we examine the runtime of 10 alternating least squares iterations using zero-initialized coordinate descent least squares or FAST-CD least squares.

```{R}
results <- data.frame()
for(size in seq(from = 10, to = 100, by = 10)){
  cat("Calculating runtime for size: ", size, "\n")
  df <- list()
  for(replicate in 1:3){
    ab <- random_system(size, replicate)
    solution <- RcppML::nnls(ab$a, ab$b)
    x_zero <- as.matrix(rep(0, size))
    x_rand <- as.matrix(runif(length(ab$b))) * mean(solution)
    x_solve <- base::solve(ab$a, ab$b)
    df[[replicate]] <- summary(microbenchmark(
      RcppML::nnls(ab$a, ab$b, x = x_zero, cd_maxit = 100000),
      RcppML::nnls(ab$a, ab$b, fast_maxit = 100, cd_maxit = 100000),
      times = 3, unit = 's'
    ))$mean
  }
  df <- do.call(rbind, df)
  time <- colMeans(df)
  err <- apply(df, 2, sd) / time[1]
  time <- time / time[1]
  df <- data.frame(cbind(time, err))
  df$size <- size
  df$method <- c("zero", "random", "unconstrained", "FAST")
  ifelse(nrow(results) == 0, results <- df, results <- rbind(results, df))
}

ggplot(results, aes(x = size, y = time, color = method, group = method)) +
    geom_point() +
    geom_line(size = 1) +
    geom_errorbar(aes(ymin = time - err/2, ymax = time + err/2), width = 0.1) +
    theme_classic() +
    scale_y_continuous(expand = c(0.01, 0)) +
    theme(aspect.ratio = 1) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "number of variables in system", y = "runtime relative to zero-initialized")
```

## Runtime of Active Set Methods vs. Coordinate Descent

It is very clear that the Lawson-Hanson family of algorithms is inefficient for large solutions, that coordinate descent solvers are not particularly fast, and that feasible set methods such as TNT-NN suffer from dependence on heuristics and hyperparameters.

We propose to serialize the first step of the TNT-NN algorithm and the coordinate descent algorithm. This will take advantage of fast Cholesky decompositions and provide the coordinate descent solver with a very good, if not exact, starting point.

For instance, consider the runtime of unconstrained least squares, the initial phase of the TNT-NN algorithm, a coordinate descent algorithm, and a hybrid FAST-CD algorithm:

```{R}
results <- data.frame()
for(size in seq(from = 10, to = 100, by = 10)){
  cat("Calculating runtime for size: ", size, "\n")
  df <- list()
  for(replicate in 1:5){
    ab <- random_system(size, replicate)
    solution <- RcppML::nnls(ab$a, ab$b)
    x_zero <- as.matrix(rep(0, size))
    df[[replicate]] <- summary(microbenchmark(
      RcppML::nnls(ab$a, ab$b, fast_maxit = 0, cd_maxit = 0),
      RcppML::nnls(ab$a, ab$b, fast_maxit = 100, cd_maxit = 0),
      RcppML::nnls(ab$a, ab$b, x = x_zero, fast_maxit = 0, cd_maxit = 10000),
      RcppML::nnls(ab$a, ab$b, fast_maxit = 100, cd_maxit = 10000),
      times = 5
    ))$median
  }
  df <- do.call(rbind, df)
  time <- colMeans(df)
  err <- apply(df, 2, sd) / time[1]
  time <- time / time[1]
  df <- data.frame(cbind(time, err))
  df$size <- size
  df$method <- c("LLT", "FAST", "CD", "FAST-CD")
  ifelse(nrow(results) == 0, results <- df, results <- rbind(results, df))
}

ggplot(results, aes(x = size, y = time, color = method, group = method)) +
    geom_point() +
    geom_line(size = 1) +
    geom_errorbar(aes(ymin = time - err/2, ymax = time + err/2), width = 0.1) +
    theme_classic() +
    scale_y_continuous(expand = c(0.01, 0), trans = 'log') + 
    theme(aspect.ratio = 1) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "number of variables in system", y = "runtime relative to\nunconstrained solution")
```

Note that FAST-CD converges very quickly compared to coordinate descent, and that the hybrid FAST-CD algorithm is always faster than Coordinate Descent alone. However, the advantage is not very great, which is due to simulating extremely ill-conditioned systems at random.

We can simulate well-conditioned systems by running matrix factorization, in this case to a tolerance of 0.001, and then examine runtime for each method:

```{R}
results <- data.frame()
for(size in seq(from = 10, to = 60, by = 10)){
  cat("Calculating runtime for size: ", size, "\n")
  df <- list()
  for(replicate in 1:5){
    set.seed(replicate)
    A <- rsparsematrix(500, 1000, 0.25)
    w <- nmf(A, size, verbose = FALSE, maxit = 1)$w
    df[[replicate]] <- summary(microbenchmark(
      RcppML::project(A, w, fast_maxit = 0, cd_maxit = 0),
      RcppML::project(A, w, fast_maxit = 100, cd_maxit = 0),
      RcppML::project(A, w, fast_maxit = 0, cd_maxit = 10000),
      RcppML::project(A, w, fast_maxit = 100, cd_maxit = 10000),
      times = 5
    ))$median
  }
  df <- do.call(rbind, df)
  time <- colMeans(df)
  err <- apply(df, 2, sd) / time[1]
  time <- time / time[1]
  df <- data.frame(cbind(time, err))
  df$size <- size
  df$method <- c("LLT", "FAST", "CD", "FAST-CD")
  ifelse(nrow(results) == 0, results <- df, results <- rbind(results, df))
}

ggplot(results, aes(x = size, y = time, color = method, group = method)) +
    geom_point() +
    geom_line(size = 1) +
    geom_errorbar(aes(ymin = time - err/2, ymax = time + err/2), width = 0.1) +
    theme_classic() +
    scale_y_continuous(expand = c(0.01, 0), trans = 'log') + 
    theme(aspect.ratio = 1) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "number of variables in system", y = "runtime relative to\nunconstrained solution")
```

```{R}
A <- rsparsematrix(1000, 10000, 0.1)
autoplot(microbenchmark(
  "cd only" = nmf(A, 10, fast_maxit = 0, cd_maxit = 1000, verbose = F, maxit = 1000),
  "fast only" = nmf(A, 10, fast_maxit = 1000, cd_maxit = 0, verbose = F, maxit = 1000),
  "fast-cd" = nmf(A, 10, fast_maxit = 1000, cd_maxit = 1000, verbose = F, maxit = 1000),
  "svd" = irlba(A, 10),
  times = 20
))
```

## Panel C. Runtime of Coordinate Descent with varying initializations

```{R}
size <- 50
ab <- random_system(50, 123)
autoplot(microbenchmark(
    "zero-init CD" = RcppML::nnls(ab$a, ab$b, x = as.matrix(rep(0, 50)), cd_maxit = 1000),
    "FAST only" = RcppML::nnls(ab$a, ab$b, cd_maxit = 0),
    "CD after LLT" = RcppML::nnls(ab$a, ab$b, fast_maxit = 0, cd_maxit = 1000),
    "base-solve" = base::solve(ab$a, ab$b),
    "crossprod(X)" = crossprod(ab$X),
    "nnls::nnls" = nnls::nnls(ab$X, ab$y),
    "fnnls" = multiway::fnnls(ab$a, ab$b),
    "fast-cd" = RcppML::nnls(ab$a, ab$b),
    times = 100
))
```

## Full Solution Path for FAST + CD NNLS

We will put all solutions for each iteration in the `solutions` list. Iterations begin with unconstrained solution, then an iteration for each FAST step.

For the FAST iterations, we will use the above R function.

```{R}
set.seed(2)
m <- 10
X <- abs(matrix(rnorm(100*m),100, m))
a <- crossprod(X)
b <- runif(m)
solutions <- list()
solutions[[1]] <- fast_nnls(a, b, maxit = 0)
solutions[[2]] <- fast_nnls(a, b, maxit = 1)
solutions[[3]] <- fast_nnls(a, b, maxit = 2)
solutions[[4]] <- fast_nnls(a, b, maxit = 3)
solutions <- as.data.frame(solutions)
colnames(solutions) <- c("LS", "FAST 1", "FAST 2", "FAST 3")

rownames(solutions) <- paste0("var.", 1:nrow(solutions))
solutions <- reshape2::melt(as.matrix(solutions))
colnames(solutions) <- c("variable", "iter", "coef")

ggplot(solutions, aes(x = iter, y = coef, color = variable, group = variable)) +
        geom_point() +
        geom_line(size = 1) +
        theme_classic() +
        scale_y_continuous(expand = c(0.01, 0)) +
        theme(aspect.ratio = 1) +
        theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
        labs(x = "iteration", y = "variable coefficient")
```

We might also solve the same solution using coordinate descent:

```{R}
solutions <- list()
set.seed(1)
x <- rep(0, 10)
solutions[[1]] <- x
for(i in seq(1, 15, 1)){
  solutions[[i + 1]] <- RcppML::solve(a, b, x = x, maxit = i, nonneg = TRUE)
}
solutions <- as.data.frame(solutions)
colnames(solutions) <- 0:15

rownames(solutions) <- paste0("var.", 1:nrow(solutions))
solutions <- reshape2::melt(as.matrix(solutions))
colnames(solutions) <- c("variable", "iter", "coef")

ggplot(solutions, aes(x = iter, y = coef, color = variable, group = variable)) +
        geom_point() +
        geom_line(size = 1) +
        theme_classic() +
        scale_y_continuous(expand = c(0.01, 0)) +
        theme(aspect.ratio = 1) +
        theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
        labs(x = "iteration", y = "variable coefficient")

```

