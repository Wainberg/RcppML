---
title: "Robust NMF with random initializations"
author: "Zach DeBruine"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with NMF}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Non-negative matrix factorization (NMF) is NP-hard ([Vavasis, 2007](https://arxiv.org/abs/0708.4149)). As such, the best that NMF can do, in practice, is find the best discoverable local minima from some set of initializations.

Non-negative Double SVD (NNDSVD) has previously been proposed as a "head-start" for NMF ([Boutsidis, 2008](https://www.sciencedirect.com/science/article/abs/pii/S0031320307004359)), but never does better than random initializations and often does worse (see my [blog post](https://www.zachdebruine.com/post/learning-optimal-nmf-models-from-random-restarts/) on this).

## Random initialization

Random initializations are the best initializations because they are not local minima, unlike SVD-based initializations. There are two types of random initialization which work well for NMF:

* uniform distributions. (i.e. `runif(min, max)`)
* normal distributions (i.e. `rnorm(mean, sd)`)

In general, the type of data dicates which type of initialization will discover the best minima. Here are a couple takeaways:

* very large data (>10,000 samples and features) often needs few random restarts -- the best solutions are found repeatedly.
* small, clean data benefits from `rnorm`
* small, dirty data benefits from `runif`

To make things even more subjective and complicated, the bounds of the uniform distribution can affect the best discoverable solution.

## Random restarts with RcppML

Let's see how multiple restarts can improve a model. In RcppML, we simply specify multiple seeds to run multiple restarts:

```{R, message = FALSE, warning = FALSE}
library(RcppML)
data(hawaiibirds)
m1 <- nmf(hawaiibirds$counts, k = 10, seed = 1)
m2 <- nmf(hawaiibirds$counts, k = 10, seed = 1:10)
```

```{R}
evaluate(m1, hawaiibirds$counts)
```

```{R}
evaluate(m2, hawaiibirds$counts)
```

Multiple random restarts were able to lower the error of the model! 

What happens when only a single seed is specified? RcppML uses `runif` to generate a uniform distribution in a randomly selected range. `rnorm` is not used just to be safe, because sometimes `rnorm` can actually give worse results.

What happens when multiple seeds are specified? RcppML generates multiple initializations, for each initialization randomly choosing to use `runif` or `rnorm`, and then randomly selecting a uniform range or mean and standard deviation, respectively.

## Refining NMF models from warm starts

Suppose we learn an NMF model and want to come back later and fit it further. Alternatively, we might learn a model from one set of samples, and simply want to fit it to another set of samples.

Here we learn an NMF model on half of all movie users in the `movielens` dataset, and then fit it further on the other half:

```{R}
data(movielens)
A <- movielens$ratings
users1 <- sample(1:ncol(A), floor(ncol(A)/2))
users2 <- (1:ncol(A))[-users1]

model1 <- nmf(A[, users1], k = 7, mask = "zeros", seed = 1:10)
model2 <- nmf(A[, users2], k = 7, mask = "zeros", seed = model1@w)

cat("model 1 iterations: ", model1@misc$iter,
    ", model 2 iterations: ", model2@misc$iter)
```

Here use used model1 as a "warm start" for training model2, and thus not as many iterations were needed to refine model2. But suppose we trained model2 from a "cold start", would we have done better (as measured by MSE)?

```{R}
model2_new <- nmf(A[, users2], k = 7, mask = "zeros", seed = 1:10)

cat("model 2 warm-start", evaluate(model2_new, A[, users2], mask = "zeros"),
    ", model 2 random start: ", evaluate(model2, A[, users2], mask = "zeros"))
```

Surprisingly, we don't do better! This, of course, could be different depending on the dataset and training methodology.